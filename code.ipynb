{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nsUoMMNDVrxr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "\n",
        "  d_k = q.size()[-1]\n",
        "  scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "  print(f\"scaled.size() : {scaled.size()}\")\n",
        "  if mask is not None:\n",
        "\n",
        "\n",
        "    print(f\"\\n-- Adding Mask of shape {mask.size()} --\\n\")\n",
        "    scaled+=mask\n",
        "  attention = F.softmax(scaled, dim=-1)\n",
        "  values = torch.matmul(attention, v)\n",
        "\n",
        "  return values, attention"
      ],
      "metadata": {
        "id": "Et9Jfe54V0Sb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, num_heads):\n",
        "\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_model//num_heads\n",
        "    self.qkv_layer = nn.Linear(d_model, 3*d_model)\n",
        "    self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "\n",
        "    batch_size, sequence_length, d_model = x.size()\n",
        "    print(f\"x.size(): {x.size()}\")\n",
        "    qkv = self.qkv_layer(x)\n",
        "    print(f\"qkv.size(): {qkv.size()}\")\n",
        "    qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3*self.head_dim)\n",
        "    print(f\"qkv.size(): {qkv.size()}\")\n",
        "    qkv = qkv.permute(0, 2, 1, 3)\n",
        "    print(f\"after permute - qkv.size(): {qkv.size()}\")\n",
        "    q,k,v = qkv.chunk(3, dim=-1)\n",
        "    print( f\"q,k,v sizes: {q.size()} \" )\n",
        "    values, attention = scaled_dot_product(q, k, v, mask)\n",
        "    print(f\"values.size(): {values.size()},\\nattention.size:{ attention.size()} \")\n",
        "    values = values.reshape(batch_size, sequence_length, self.num_heads*self.head_dim)\n",
        "    print(f\"values.size(): {values.size()}\")\n",
        "    out = self.linear_layer(values)\n",
        "    print(f\"out.size(): {out.size()}\")\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "DH1C9N3hVyAE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, max_sequence_length):\n",
        "    super().__init__()\n",
        "    self.max_sequence_length = max_sequence_length\n",
        "    self.d_model = d_model\n",
        "\n",
        "\n",
        "  def forward(self):\n",
        "\n",
        "    pos = torch.arange(0, self.d_model, 2).float()\n",
        "    denominator = torch.pow(10000, pos/self.d_model)\n",
        "    position = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
        "\n",
        "    even_PE = torch.sin(position/denominator)\n",
        "    odd_PE = torch.cos(position/denominator)\n",
        "    stacked = torch.stack( [even_PE, odd_PE], dim=2)\n",
        "    PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "\n",
        "    return PE"
      ],
      "metadata": {
        "id": "rRsEVmmAmPHT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "  def __init__(self, parameters_shape, eps=1e-5):\n",
        "      super().__init__()\n",
        "      self.parameters_shape=parameters_shape\n",
        "      self.eps=eps\n",
        "      self.weights = nn.Parameter(torch.ones(parameters_shape))\n",
        "      self.bias =  nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "\n",
        "  def forward(self, inputs):\n",
        "\n",
        "      dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
        "      mean = inputs.mean(dim=dims, keepdim=True)\n",
        "      print(f\"Mean ({mean.size()})\")\n",
        "      var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
        "      std = (var + self.eps).sqrt()\n",
        "      print(f\"Standard Deviation  ({std.size()})\")\n",
        "      y = (inputs - mean) / std\n",
        "      print(f\"y: {y.size()}\")\n",
        "      out = self.weights * y  + self.bias\n",
        "      print(f\"self.gamma: {self.weights.size()}, self.beta: {self.bias.size()}\")\n",
        "      print(f\"out: {out.size()}\")\n",
        "\n",
        "      return out"
      ],
      "metadata": {
        "id": "7GqodV9_Kt3y"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "    super(PositionwiseFeedForward, self).__init__()\n",
        "    self.linear1 = nn.Linear(d_model, hidden)\n",
        "    self.linear2 = nn.Linear(hidden, d_model)\n",
        "    self.relu =  nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    print(f\"x after first linear layer: {x.size()}\")\n",
        "    x = self.relu(x)\n",
        "    print(f\"x after activation: {x.size()}\")\n",
        "    x = self.dropout(x)\n",
        "    print(f\"x after dropout: {x.size()}\")\n",
        "    x = self.linear2(x)\n",
        "    print(f\"x after 2nd linear layer: {x.size()}\")\n",
        "    return x"
      ],
      "metadata": {
        "id": "C4dyb4hcQAnW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual_x = x\n",
        "        x = self.attention(x, mask=None)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + residual_x)\n",
        "        residual_x = x\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + residual_x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "NNUfZYIxTHX6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual_x = x\n",
        "        print(\"------- Encoder Multi-Head-Attention ------\", '\\n')\n",
        "        x = self.attention(x, mask=None)\n",
        "        print('\\n')\n",
        "        print( \"------- Applying Dropout ------\")\n",
        "        print('\\n')\n",
        "        x = self.dropout1(x)\n",
        "        print( \"------- Addition & Normalization Layer-1 ------\", '\\n')\n",
        "        x = self.norm1(x + residual_x)\n",
        "        residual_x = x\n",
        "        print('\\n')\n",
        "        print(  \"------- Encoder Feed Forward Neural Network ------\",'\\n')\n",
        "        x = self.ffn(x)\n",
        "        print('\\n')\n",
        "        print(\"------- Applying Dropout ------\")\n",
        "        x = self.dropout2(x)\n",
        "        print('\\n')\n",
        "        print(\"------- Addition & Normalization Layer-2 ------\",'\\n')\n",
        "        x = self.norm2(x + residual_x)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
        "                                     for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "tRZZGuMVWDks"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadCrossAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "\n",
        "      super().__init__()\n",
        "      self.d_model = d_model\n",
        "      self.num_heads = num_heads\n",
        "      self.head_dim = d_model//num_heads\n",
        "      self.kv_layer = nn.Linear(d_model, 2*d_model)\n",
        "      self.q_layer = nn.Linear(d_model, d_model)\n",
        "      self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "\n",
        "    def forward(self, x, y, mask=None):\n",
        "\n",
        "      batch_size, sequence_length, d_model = x.size()\n",
        "      print(f\"x.size(): {x.size()}\")\n",
        "\n",
        "      kv = self.kv_layer(x)\n",
        "      q = self.q_layer(y)\n",
        "      print(f\"kv.size(): {kv.size()}\")\n",
        "      print(f\"q.size(): {q.size()}\")\n",
        "\n",
        "      kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2*self.head_dim)\n",
        "      q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
        "      print(f\"kv.size(): {kv.size()}\")\n",
        "      print(f\"q.size(): {q.size()}\")\n",
        "\n",
        "      kv = kv.permute(0, 2, 1, 3)\n",
        "      q = q.permute(0, 2, 1, 3)\n",
        "      print(f\"after permute - kv.size(): {kv.size()}\")\n",
        "      print(f\"after permute - q.size(): {q.size()}\")\n",
        "\n",
        "      k,v = kv.chunk(2, dim=-1)\n",
        "      print( f\"k,v sizes: {k.size()} \" )\n",
        "\n",
        "      values, attention = scaled_dot_product(q, k, v, mask)\n",
        "      print(f\"values.size(): {values.size()}\" )\n",
        "      print( f\"attention.size:{ attention.size()}\" )\n",
        "\n",
        "      values =  values.permute(0, 2, 1, 3)\n",
        "      values = values.reshape(batch_size, sequence_length, d_model)\n",
        "      out = self.linear_layer(values)\n",
        "      print(f\"values.size(): {values.size()}\")\n",
        "      print(f\"out.size(): {out.size()}\")\n",
        "\n",
        "      return out"
      ],
      "metadata": {
        "id": "1ACMZFuoVSkS"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "    self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "    self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "    self.norm3 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout3 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "\n",
        "  def forward(self, x, y, decoder_mask):\n",
        "\n",
        "    residual_y = y\n",
        "    print('\\n')\n",
        "    print(\"----- Decoder Masked-Multi-Head-Attention -----\", '\\n')\n",
        "    y = self.self_attention(y, mask=decoder_mask)\n",
        "    print('\\n')\n",
        "    print('----- Applying Dropout ------')\n",
        "    print('\\n')\n",
        "    y = self.dropout1(y)\n",
        "    print('----- Addition & Normmalization Layer-1 -----', '\\n')\n",
        "    y = self.norm1(y+residual_y)\n",
        "    residual_y = y\n",
        "    print('\\n')\n",
        "    print('------ Decoder Cross-Multi-Head-Atention ------', '\\n')\n",
        "    y = self.encoder_decoder_attention(x, y, mask=None)\n",
        "    print('\\n')\n",
        "    print('------ Applying Dropout ------')\n",
        "    print('\\n')\n",
        "    y = self.dropout2(y)\n",
        "    print('----- Addition & Normmalization Layer-2 -----', '\\n')\n",
        "    y = self.norm2(y+residual_y)\n",
        "    residual_y = y\n",
        "    print('\\n')\n",
        "    print('------ Feed Forward Neural Neywork ------', '\\n')\n",
        "    y = self.ffn(y)\n",
        "    print('\\n')\n",
        "    print('----- Applying Dropout ------')\n",
        "    print('\\n')\n",
        "    y = self.dropout3(y)\n",
        "    print('----- Addition & Normmalization Layer-3 -----', '\\n')\n",
        "    y = self.norm3(y+residual_y)\n",
        "\n",
        "    return y\n"
      ],
      "metadata": {
        "id": "6wTnV_YzafEy"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SequentialDecoder(nn.Sequential):\n",
        "  def forward(self, *inputs):\n",
        "    x, y, mask = inputs\n",
        "    for module in self._modules.values():\n",
        "      y = module(x, y, mask)\n",
        "    return y"
      ],
      "metadata": {
        "id": "YQ1HM1Kzc94N"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers=1):\n",
        "    super().__init__()\n",
        "    self.layers  = SequentialDecoder(*[ DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob )\n",
        "                                        for _ in range(num_layers)])\n",
        "\n",
        "  # Fix: Correct the indentation of the forward method\n",
        "  def forward(self, x, y, mask):\n",
        "    y = self.layers(x, y, mask)\n",
        "    return y"
      ],
      "metadata": {
        "id": "FRHEUAuAeB5e"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = torch.randn( (batch_size, max_sequence_length, d_model) ) # Positional Encoding of input label\n",
        "# y = torch.randn( (batch_size, max_sequence_length, d_model) ) # Positional Encoding of output label\n",
        "# mask = torch.full([max_sequence_length, max_sequence_length] , float('-inf'))\n",
        "# mask = torch.triu(mask, diagonal=1)\n",
        "# encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\n",
        "# out = encoder(x)\n",
        "# decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\n",
        "# out = decoder(x, y, mask)"
      ],
      "metadata": {
        "id": "vhGT6FujfD27"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_encoder_layers, num_decoder_layers, max_sequence_length, vocab_size):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_encoder_layers)\n",
        "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_decoder_layers)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.final_layer = nn.Linear(d_model, vocab_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, y, mask):\n",
        "        x = x + self.positional_encoding()\n",
        "        y = y + self.positional_encoding()\n",
        "        encoder_output = self.encoder(x)\n",
        "        decoder_output = self.decoder(encoder_output, y, mask)\n",
        "        logits = self.final_layer(decoder_output)\n",
        "        final_output = self.softmax(logits)\n",
        "        return decoder_output, final_output\n"
      ],
      "metadata": {
        "id": "lbOF9gRaEh95"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "batch_size = 30\n",
        "max_sequence_length = 200\n",
        "ffn_hidden = 2048\n",
        "num_encoder_layers = 1\n",
        "num_decoder_layers = 1\n",
        "vocab_size = 10000\n",
        "\n",
        "x = torch.randn((batch_size, max_sequence_length, d_model))  # input sequence\n",
        "y = torch.randn((batch_size, max_sequence_length, d_model))  # target sequence\n",
        "mask = torch.full([max_sequence_length, max_sequence_length], float('-inf'))\n",
        "mask = torch.triu(mask, diagonal=1)\n",
        "\n",
        "transformer = Transformer(d_model, ffn_hidden, num_heads, drop_prob, num_encoder_layers, num_decoder_layers, max_sequence_length, vocab_size)\n",
        "decoder_output, final_output = transformer(x, y, mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58tWwMz9Fr5t",
        "outputId": "40800408-9a8e-4233-ca70-b07a32c79a45"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------- Encoder Multi-Head-Attention ------ \n",
            "\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "after permute - qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q,k,v sizes: torch.Size([30, 8, 200, 64]) \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]),\n",
            "attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "\n",
            "\n",
            "------- Applying Dropout ------\n",
            "\n",
            "\n",
            "------- Addition & Normalization Layer-1 ------ \n",
            "\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "\n",
            "\n",
            "------- Encoder Feed Forward Neural Network ------ \n",
            "\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "\n",
            "\n",
            "------- Applying Dropout ------\n",
            "\n",
            "\n",
            "------- Addition & Normalization Layer-2 ------ \n",
            "\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "\n",
            "\n",
            "----- Decoder Masked-Multi-Head-Attention ----- \n",
            "\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "after permute - qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q,k,v sizes: torch.Size([30, 8, 200, 64]) \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "\n",
            "-- Adding Mask of shape torch.Size([200, 200]) --\n",
            "\n",
            "values.size(): torch.Size([30, 8, 200, 64]),\n",
            "attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "\n",
            "\n",
            "----- Applying Dropout ------\n",
            "\n",
            "\n",
            "----- Addition & Normmalization Layer-1 ----- \n",
            "\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "\n",
            "\n",
            "------ Decoder Cross-Multi-Head-Atention ------ \n",
            "\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "kv.size(): torch.Size([30, 200, 1024])\n",
            "q.size(): torch.Size([30, 200, 512])\n",
            "kv.size(): torch.Size([30, 200, 8, 128])\n",
            "q.size(): torch.Size([30, 200, 8, 64])\n",
            "after permute - kv.size(): torch.Size([30, 8, 200, 128])\n",
            "after permute - q.size(): torch.Size([30, 8, 200, 64])\n",
            "k,v sizes: torch.Size([30, 8, 200, 64]) \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64])\n",
            "attention.size:torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "\n",
            "\n",
            "------ Applying Dropout ------\n",
            "\n",
            "\n",
            "----- Addition & Normmalization Layer-2 ----- \n",
            "\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "\n",
            "\n",
            "------ Feed Forward Neural Neywork ------ \n",
            "\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "\n",
            "\n",
            "----- Applying Dropout ------\n",
            "\n",
            "\n",
            "----- Addition & Normmalization Layer-3 ----- \n",
            "\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Decoder Output Shape:- ', decoder_output.size())\n",
        "print('Final Output Shape:- ', final_output.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSg1k5PrGkpQ",
        "outputId": "25c81355-9122-48d2-bcdf-b85a534ebfe0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Output Shape:-  torch.Size([30, 200, 512])\n",
            "Final Output Shape:-  torch.Size([30, 200, 10000])\n"
          ]
        }
      ]
    }
  ]
}